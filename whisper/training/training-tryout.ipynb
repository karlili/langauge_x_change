{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee342d21",
   "metadata": {},
   "source": [
    "the original notebook is from hugging face colab notebook here (https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)\n",
    "\n",
    "\n",
    "Make sure you have the following dependencies installed in your environment\n",
    "\n",
    "```\n",
    "pip install datasets transformers evaulate jiwer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1234f2",
   "metadata": {},
   "source": [
    "the common voice dataset is coming from here (https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/zh-HK/train?p=84)\n",
    "\n",
    "It is part of mozilla foundation common voice project\n",
    "\n",
    "----\n",
    "\n",
    "We could use this format to prepare our own dataset to fine tune our version of whisper\n",
    "\n",
    "----\n",
    "\n",
    "If you want to re-use / avoid to download the voice file every time, you can un-comment the part which specify `cache_dir` and point it to the directory you want those file to be downloaded / already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13205eb410405fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T20:09:39.997469Z",
     "start_time": "2023-11-21T20:02:41.345262Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 5636\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 2565\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append(datasets_dir)\n",
    "\n",
    "# before downloading any new dataset, \n",
    "# make sure to check if it needs to Check and Agrees to the terms first, otherwise the download would fail\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset_name = \"mozilla-foundation/common_voice_15_0\"\n",
    "language_to_train = 'yue'\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"train+validation\",\n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/datasets\"\n",
    "  )\n",
    "\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"test\",  \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/datasets\"\n",
    "  )\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93cb70f8c8df1663",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip install \"tokenizers>=0.14,<0.15\"\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/feature\"\n",
    "  ) # start with the whisper small checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db962245-70f8-4288-9d9d-dc6144d90cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"cantonese\", \n",
    "task=\"transcribe\",\n",
    "cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "002a1f20-0b5a-4861-92bb-ee5aceb2f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"cantonese\", \n",
    "task=\"transcribe\",\n",
    "cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/processor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d8a338c-6ce8-4b08-866f-dad714ba24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_id': '2ecfe4e00a829397a04e316949bf3058c9ed72b0da9fad2686b0bc3bd98654d8a586e878cc3aa7609bf0359f56e24b3bc0f6f1ec4d1ec958e569bbaaf742560b', 'path': '/Volumes/BACKUP/Coding/HUGGING_FACE/datasets/downloads/extracted/5f8c376b62cbcec81f092e38c43f1519f67645668f8044d9b7c5a51c4297c524/yue_train_0/common_voice_yue_31210647.mp3', 'audio': {'path': '/Volumes/BACKUP/Coding/HUGGING_FACE/datasets/downloads/extracted/5f8c376b62cbcec81f092e38c43f1519f67645668f8044d9b7c5a51c4297c524/yue_train_0/common_voice_yue_31210647.mp3', 'array': array([ 1.45519152e-10,  4.36557457e-11,  4.36557457e-11, ...,\n",
      "       -2.06303957e-06, -1.26592931e-06,  1.36844028e-06]), 'sampling_rate': 16000}, 'sentence': 'ç‡å…§å®¹é•·çŸ­å˜…', 'up_votes': 4, 'down_votes': 0, 'age': 'teens', 'gender': 'male', 'accent': 'é¦™æ¸¯ç²µèª', 'locale': 'yue', 'segment': '', 'variant': ''}\n"
     ]
    }
   ],
   "source": [
    "# Preparing Data\n",
    "\n",
    "# Whisper expecting the audio to be at sampling rate @16000 - this is just to make sure the sampling rate fits whisper's training\n",
    "# Since our input audio is sampled at 48kHz, we need to downsample it to 16kHz prior to passing it to the Whisper feature extractor, \n",
    "# 16kHz being the sampling rate expected by the Whisper model.\n",
    "from datasets import Audio\n",
    "raw_common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(raw_common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd3747",
   "metadata": {},
   "source": [
    "prepare the dataset\n",
    "doing the encoding -> preparing the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e69ad91a-725f-40a6-8dfd-7e3c64af477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5636/5636 [08:57<00:00, 10.49 examples/s]\n",
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2565/2565 [04:05<00:00, 10.46 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5636\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2565\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "finalized_common_voice = raw_common_voice.map(prepare_dataset, \n",
    "  remove_columns=raw_common_voice.column_names[\"train\"], \n",
    "  num_proc=2)\n",
    "print(finalized_common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59378d24",
   "metadata": {},
   "source": [
    "the following is the actual training and evaluation of the model\n",
    "\n",
    "using the trainer provided by huggingface\n",
    "\n",
    "Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric. We need to define a compute_metrics function that handles this computation.\n",
    "\n",
    "Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "Define the training configuration: this will be used by the ğŸ¤— Trainer to define the training schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2be9c5d3-35df-4995-aa04-eea6fa1b1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b87322a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acfbcd",
   "metadata": {},
   "source": [
    "Evaluation using hugging face metric - WER (Word error rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47506d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (1.26.2)\n",
      "Requirement already satisfied: dill in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.19.4)\n",
      "Requirement already satisfied: packaging in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (23.2)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f3bcbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.49k/4.49k [00:00<00:00, 12.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1da949c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/models\"\n",
    "  )\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08eb7b",
   "metadata": {},
   "source": [
    "What should be the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a nice youtube video guide / introduction for how to use tensorboard (https://www.youtube.com/watch?v=VJW9wU-1n18&t=4s)\n",
    "!pip install tensorboardx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d97c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H%M\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"model/whisper-small-cantonese_\"+now,  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,  # if we are not using CUDA or non graphics card, use fp16=false\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"], #this would requires the tensorboardx to be installed\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=finalized_common_voice[\"train\"],\n",
    "    eval_dataset=finalized_common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    # checkpoint_activations=True\n",
    ")\n",
    "\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e159c",
   "metadata": {},
   "source": [
    "The actual Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0bc840b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 8:06:49, Epoch 11/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.157449</td>\n",
       "      <td>69.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.171989</td>\n",
       "      <td>63.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.181519</td>\n",
       "      <td>61.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.186846</td>\n",
       "      <td>61.307692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.05433571213460527, metrics={'train_runtime': 29224.9012, 'train_samples_per_second': 2.19, 'train_steps_per_second': 0.137, 'total_flos': 1.843137234763776e+19, 'train_loss': 0.05433571213460527, 'epoch': 11.33})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b87e",
   "metadata": {},
   "source": [
    "if you need to push the model to hugging face hub, run the following block\n",
    "\n",
    "```\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is optional. but it would allow you to upload the model to hugging face space later on\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to push\n",
    "\n",
    "#the following arguments are needed only when we are pushing the model to hugging face hub\n",
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: hi, split: test\",\n",
    "    \"language\": \"Cantonese\",\n",
    "    \"model_name\": \"[language-x-change] Custom Whisper for Cantanese\",  # a 'pretty' name for our model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895521a",
   "metadata": {},
   "source": [
    "The following is only needed when we want to deploy a runnable version with our uploaded model on hugging face spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a427339",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"your-own-model\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Hindi\",\n",
    "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531bd73",
   "metadata": {},
   "source": [
    "to use the model we just compiled (https://huggingface.co/docs/transformers/tasks/asr#inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37a6db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - {'text': ' ä¸­ä¸Šç’°è¿‘åŠå±±ä¸€å¸¶ é™¤å’—ç•€äººå€‹æ„Ÿè¦ºéˆæ€§ä¹‹å¤– å‘¢åº¦å˜…å»ºç¯‰ç‰©äº¦éƒ½ä¿‚æ–°å¤ äº¤èã— å””è¬›ä½ å””çŸ¥ å‘¢åº¦æ›¾ç¶“å‡ºç¾å—°å€‹å«åšä¸‰æ—¥é–“å˜…ç¤¾å€å•²'}\n",
      "[1] - {'text': ' è¢«ç¨±ç‚ºæ²™äºé–“å˜…ç¤¾å€ ç›®å‰ä¸¦å†‡å®Œæ•´å˜…æ–‡ç»è¨˜éŒ„è³‡æ–™ ä¸Šå‚³å–ºåä¹ä¸–ç´€ å£«ä¸¹é “è¡—åŒåŸ‹å¿…çƒˆè€…æ™‚é–“é™„è¿‘ä¸€å¸¶å˜…è¯äººèšå±…åœ° æ‰€å»ºé€ å˜…ä¸‰åé–“çŸ³å±‹è€Œå¾—å'}\n",
      "[2] - {'text': 'éš¨ä½ç¤¾æœƒè®Šé· çŸ³å±‹å·²ç¶“ä¸å¾©å­˜åœ¨ç¾æ™‚ æ–¹ä½å…§ä»æœ‰æ•¸åº§ å¤§ç´„å–ºä¸€ä¹äº”é›¶å¹´ä»£å»ºæˆå˜…å”æµå»ºç¯‰åˆ†åˆ¥ä¿‚å–ºäºŒé›¶ä¸€ä¹å¹´ ç¢ºå®šæˆç‚ºäºŒç´šæ­·å²å»ºç¯‰å˜…å²ä¸¹é “è¡—å…«åå…«åŠä¹åè™ŸåŒåŸ‹å¹³ç´šæœ‰ä»£è˜‹æœå˜…è©±è¨€æ–¹è¥¿å”æµå»ºç¯‰ç¾¤'}\n",
      "[3] - {'text': ' Carol,å…¶å¯¦ç•¶åˆä¸‰åé–“å€‹èµ·æºä¿‚'}\n",
      "[4] - {'text': 'å…¶å¯¦å¦‚æœæ¾è¿”è³‡æ–™çš„è©± æœ€æ—©å…¶å¯¦æˆ‘å“‹ä¿‚å–ºä¸€ç™¾å…«é›¶å¹´å˜…æ”¿åºœæ†²å ±åº¦è¦‹åˆ°ä¸‰åé–“å‘¢å€‹åå˜… å› ç‚ºæˆ‘å“‹è€Œå®¶å–ºé¦™æ¸¯å˜…åœ°åœ–ä¸Šé¢ å…¶å¯¦æˆ‘å“‹éƒ½å¥½é›£å¯ä»¥è¦‹åˆ°ä¸‰åé–“å‘¢å€‹åå˜…å–‡ å·²ç¶“'}\n",
      "[5] - {'text': 'çœŸä¿‚çŸ¥é“å‘¢å€‹åå˜…äººå‘¢ å¤§æ¦‚éƒ½å·²ç¶“å»åˆ°å…­åæ­²æˆ–è€…ä»¥ä¸Šå˜…äººå…ˆè‡³æœƒè­˜å¾—ç”¨å‘¢å€‹å'}\n",
      "[6] - {'text': 'ç•¶æ™‚å…¶å¯¦ä¿‚å‘¢å€‹ä½ç½®æ‡‰è©²å°±ä¿‚èµ·å’—å¤§ç´„ä¸‰åé–“'}\n",
      "[7] - {'text': 'å¦‚æœè‚‰çœ¼è¦‹åˆ°å˜…ç—•è·¡å…¶å¯¦å¯èƒ½åªä¿‚å¾—è¿”'}\n",
      "[8] - {'text': 'å³ä¿‚å‘¢ä¸€åº¦ä¸‰åé–“è¡—åŠå„’è˜­æœƒå‘¢å€‹æ‹›ç‰Œ'}\n",
      "[9] - {'text': 'å¯èƒ½å°±ä¿‚å”¯ä¸€æˆ‘å“‹å¯ä»¥åæ˜ åˆ°ä»¥å‰å‘¢åº¦çœŸä¿‚å«åˆ°æ²™äºé–“å˜…ä¸€æ¨£å˜¢å’ä¸Šç’°ä»¥å‰å…¶å¯¦æœ‰å¥½å¤šè¯äººæœ€é«˜å˜…åœ°æ–¹åšŸ'}\n",
      "[10] - {'text': 'ä½¢å“‹å…¶å¯¦ä¿‚ä¸€ç­å±…æ°‘'}\n",
      "[11] - {'text': 'çµ„ç¹”å‡ºä¾†å˜…ä¸€å€‹åœ°æ–¹åšŸ'}\n",
      "[12] - {'text': 'å„’è˜­æ€§æœƒ å…¶å¯¦å°æ–¼ä¸€å€‹è¯äººç¤¾æœƒä¾†è¬›ä¿‚éå¸¸ä¹‹é‡è¦å•¦è¶…åˆ°ä¸€å•²å­¤é­‚å˜…é¬¼ ä»¤åˆ°å‘¢åº¦å˜…å¯èƒ½å‚·ç ´ è¡—åŠå¯ä»¥å®‰å¿ƒå•² å’æ¨£å˜…ä¸€å€‹å‚³çµ±ç¿’ä¿—å•¦'}\n",
      "[13] - {'text': 'å–ºè¡—é“ä½ˆå±€ä¸Šé¢ä¸‰åé–“ç¤¾å€å˜…ç‰¹è‰²ä¿‚é»å˜…'}\n",
      "[14] - {'text': 'å…¶å¯¦å¦‚æœæƒ³ç†è§£ä¸‰åé–“å˜…ç¯„åœå…¶å¯¦æˆ‘å“‹æ‡‰è©²ç”±ä¸‹é¢å˜…æ™‚å–®é “è¡—é–‹å§‹è¨ˆå•¦å—°å€‹å…¶å¯¦ä¿‚å€‹ä¿—å¿ƒ'}\n",
      "[15] - {'text': 'è·Ÿä½å°±ä¸€è·¯æ‰“ä¸Šå»åˆ°ä¸Šé¢åŠå±±å˜…å …åˆ°ç¯„åœ'}\n",
      "[16] - {'text': 'ä¸­é–“è¨˜æ†¶ä¸€å€‹ç¯„åœå…¶å¯¦æˆ‘å“‹éƒ½å¯ä»¥ç†è§£ç‚ºä¸‰åé–“'}\n",
      "[17] - {'text': 'åœ¨å‘¢åº¦ç•¶ä¸­è£é¢å…¶å¯¦éƒ½æœ‰å””å°‘å˜…åœ°æ–¹ å…¨éƒ¨éƒ½ä¿‚ä¸€å•²æ·¨ä¿‚äººè¡Œ è»Šå””å…¥å¾—å˜…åœ°æ–¹å•¦'}\n",
      "[18] - {'text': 'åŒ…æ‹¬ä¿‚ä¸€å•²æ–¹åŒåŸ‹ä½ å•¦ä¾‹å¦‚æ‡·å¿µæ–¹è¥¿å•¦'}\n",
      "[19] - {'text': 'æˆç‹è¡—äº¦éƒ½ä¿‚æ·¨ä¿‚äººè¡Œå˜…æ¨“æ¢¯åšŸå˜…æœ‰å•²äººäº¦éƒ½æœƒå°‡æ°¸ç†è¡—å‘¢'}\n",
      "[20] - {'text': 'è¨ˆè½å»ä¸‰åé–“å˜…ç¯„åœ'}\n",
      "[21] - {'text': 'æ‰€ä»¥å¯ä»¥è¬›å…¶å¯¦æ²™å¥¸ä¿‚ä¸€å€‹æ­¥è¡Œå˜…å°å€åšŸ'}\n",
      "[22] - {'text': 'å‘¢å€‹åœ°æ–¹å‘¢å…¶å¯¦å°±ä¿ç•™å’—å¥½å¤šå³ä¿‚å—°å€‹å¹´ä»£äº”åå¹´ä»£è‡³å…­åå¹´ä»£å•¦ä¸‰è‡³å››å±¤é«˜å˜…å¡˜æ¨“å»ºç¯‰ç‰©'}\n",
      "[23] - {'text': 'ä»Šå ´é é»æ¨£åšå…ˆå¯ä»¥ä¿ç•™åˆ°ä¸‰åé–“ç¤¾å€å˜…æ­·å²æ•…äº‹å•Š'}\n",
      "[24] - {'text': 'ä¿è‚²å…¶å¯¦æœ€å¥½å˜…æ–¹æ³•å°±ä¿‚æ´»ç”¨è½å»å•¦ã€‚å’æ‰€ä»¥å‘¢å…¶å¯¦ä¸‰åé–“å˜…é¤˜é›£æ€§æœƒå‘¢å¦‚æœå¯ä»¥ç¹¼çºŒä¸€è·¯practiceè½å»ç¹¼çºŒå¯¦è¸è½å»å˜…è©±å‘¢å°±æœƒä¿‚æœ€å¥½å˜…ä¿è‚²æ–¹æ³•'}\n",
      "[25] - {'text': 'è¿‘è¿‘å¸‚å–®é “è¡—ä¸€å¸¶å˜…å¡˜æŸ³åŸæœ¬è¢«åŠƒå…¥å¸‚å€é‡å»ºå±€å–ºäºŒé›¶é›¶ä¸‰å¹´æå‡ºå˜…é‡å»ºè¨ˆåŠƒç•¶ä¸­'}\n",
      "[26] - {'text': 'å…¶å¾Œå› æ‡‰ç¤¾å€äººå£«æå‡ºä¿ç•™å»ºç¯‰ç‰©å˜…è¨´æ±‚ï¼Œä½¿å»ºå±€å–ºäºŒé›¶äºŒé›¶å¹´æ”¾æ£„é‡å»ºè¨ˆåŠƒï¼Œç ”ç©¶ä¿è‚²åŒæ´»ç™¼é …ç›®å…¥é¢å˜…å»ºç¯‰ç¾¤'}\n",
      "[27] - {'text': 'é™¤å’—äº‹ä»¶å±€å‘¢å€‹æ´»åŒ–è¨ˆåŠƒé™„è¿‘äº¦æœ‰å¤šå€‹æ´»åŒ–æ­·å²å»ºç¯‰å˜…é …ç›®åŒ…æ‹¬å‰èº«ç‚ºè·é‡Œæ´»åº¦å‰ä»¥åˆ†è­¦å¯Ÿå®¿èˆå˜…åŸå€‰æ–¹'}\n",
      "[28] - {'text': 'åŠå‰èº«ç‚ºå¿…çƒˆé®å¸‚è¡—å¸‚å ´å˜…é¦™æ¸¯æ–°èåšè¦½é¤¨'}\n",
      "[29] - {'text': 'ä¸‰åé–“å˜…ç¯„åœå…§æˆ–å¤–é¢å…¶å¯¦éƒ½æœ‰å””å°‘å˜…æ­·å²å»ºç¯‰ç‰©å…¶å¯¦éƒ½å·²ç¶“æ´»åŒ–ç·Šã—å–‡'}\n",
      "[30] - {'text': 'ä¾‹å¦‚åŒ…æ‹¬æœ‰æ´»åŒ–æˆç‚ºåšè¦½é¤¨æˆ–è€…å¯èƒ½ä¿‚åšå’—ä¸€å•²æ–‡åŒ–æ–‡å‰µå˜…åœ°æ–¹ä½†ä¿‚æˆ‘è‡ªå·±è¦ºå¾—å…¶å¯¦å†‡ä¸€å€‹æ–¹æ³•ä¸€å®šä¿‚ä¸–ç•Œé€šè¡Œæˆ–è€…ä¿‚å°±ä¿‚å«åšä¸€å®šä¿‚æœ€å¥½'}\n",
      "[31] - {'text': 'æœ‰æ™‚å…¶å¯¦æœ‰å°‘å°‘å•†æ¥­å…ƒç´ åœ¨è£¡é¢æœ‰æ™‚éƒ½æœªå¿…ä¿‚ä¸€ä»¶å£äº‹ä¾†'}\n",
      "[32] - {'text': 'ç•¶ç„¶å•¦ä¸€å®šè¦æœ‰ä¸€å•²å•«æ³•ä¾‹å»ç´„æŸä½å—°å€‹å»ºç¯‰ç‰©å…¶å¯¦å—°å€‹æ”¹å‹•å¯ä»¥æœ‰å¹¾å¤šå•¦å™‰åˆæˆ–è€…å¯èƒ½æˆ‘å“‹æ‡‰è©²éƒ½è¦æœ‰ä¸€å•²å¥½å•²å˜…æ”¿ç­–å˜…é…å¥—å»å¹«åŠ©å‘¢å•²å»ºç¯‰ç‰©ç¹¼çºŒå»sustainæˆ‘'}\n",
      "[33] - {'text': ' In the history between the cityç™¼å±•åŠä¿ç•™èˆŠç¤¾å€åŠèˆŠå»ºç¯‰ç‰©ä¹‹é–“å¯ä»¥é»æ¨£æ‹åˆ°å€‹å¹³è¡¡å‘¢?å³ä¿‚åŸå¸‚ç™¼å±•åŠä¿è‚²å¾ä¾†éƒ½å””æ‡‰è©²ä¿‚å°ç«‹'}\n",
      "[34] - {'text': ' ä¸€å€‹èˆŠå»ºç¯‰ç‰©å…¶å¯¦å°æ–¼ä¸€å€‹ç¤¾å€å…¶å¯¦éƒ½æœ‰ä½¢å˜…åƒ¹å€¼å•¦ å¯èƒ½æœƒä¿‚'}\n",
      "[35] - {'text': 'åŒèº«ä»½èªåœ–æœ‰é—œä¿‚å•¦åˆæˆ–è€…å¯èƒ½ä½¢æœƒå¤§å‹•åˆ°ä¸€å€‹åœ°æ–¹å˜…æ–‡åŒ–æ—…éŠå•¦'}\n",
      "[36] - {'text': ' ä½¢æ¾è—ä½åŒåŸ‹ä½¢å°æ–¼å‘¢å€‹ç¤¾æœƒ å‰µé€ ç·Šå˜…åƒ¹å€¼å…¶å¯¦å‘¢éƒ½ä¿‚å¥½é‡è¦å˜…å…ƒç´ åšŸ'}\n",
      "[37] - {'text': ' Music'}\n",
      "[38] - {'text': ' The list of single-use units with a large pond-hole processing process has been completed and the construction of the city also said that it will lead to the shared-court-downloading of the landline. Hopefully, on the other hand, this place can become a smart, culture-specific and active community'}\n",
      "[39] - {'text': ' you'}\n",
      "[40] - {'text': ' BING'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "def write_contents_to_file(content): \n",
    "    now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    json_object = json.dumps(result, indent=4)\n",
    "    with open('output/'+now+\".json\", \"w\") as f:\n",
    "        f.write(json_object)\n",
    "\n",
    "path = \"model/whisper-small-cantonese_18-12-2023-22-27/checkpoint-4000\"\n",
    "processor_path = \"model/whisper-small-cantonese_18-12-2023-22-27\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "   path, \n",
    "   local_files_only=True,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(processor_path)\n",
    "\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", \n",
    "    model=model,  \n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    # chunk_length_s=20,\n",
    "    max_new_tokens=500,\n",
    "   #  batch_size=16,\n",
    "    # return_timestamps=True\n",
    "   )\n",
    "transcriber.tokenizer.get_decoder_prompt_ids(language='cantonese', task=\"transcribe\")\n",
    "\n",
    "# file_list = [\"Audio1_2.mp3\",\"Audio1_4.mp3\",\"Audio1_5.mp3\",\"Audio1_9.mp3\",\"Audio1_10.mp3\",\"Audio1_11.mp3\"]\n",
    "# for index, file in enumerate(file_list):\n",
    "#     result = transcriber(\"source/\"+file)\n",
    "#     write_contents_to_file(result)\n",
    "#     # also it will print out the result in the following output block\n",
    "#     print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "num_of_chunks = 41\n",
    "file_prefix = \"chunk\";\n",
    "file_suffix = \".mp3\"\n",
    "\n",
    "for index in range(0, num_of_chunks):\n",
    "    result = transcriber(\"source/rthk/\"+file_prefix+str(index)+file_suffix)\n",
    "    # write_contents_to_file(result)\n",
    "    print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb5b8e",
   "metadata": {},
   "source": [
    "install the following dependencies for plotting and tabulation\n",
    "\n",
    "```\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b202c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 1.4)</td>\n",
       "      <td>å…¶å¯¦éƒ½æœ‰ä½¢å˜…åƒ¹å€¼å•¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1.4, 4.68)</td>\n",
       "      <td>å¯èƒ½æœƒä¿‚åŒèº«ç²‰èªåŒæœ‰é—œä¿‚å•¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(4.68, 8.0)</td>\n",
       "      <td>åˆæˆ–è€…å¯èƒ½ä½¢æœƒå¤§å‹•åˆ°ä¸€å€‹åœ°æ–¹å˜…æ–‡åŒ–æ—…æ¸¸å•¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(8.0, 15.04)</td>\n",
       "      <td>ä½¢éš•è—ä½åŒåŸ‹ä½¢å°æ–¼å‘¢å€‹ç¤¾æœƒå‰µé€ ç·Šå˜…åƒ¹å€¼å…¶å¯¦éƒ½ä¿‚å¥½é‡è¦å˜…å…ƒç´ åšŸå˜…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(19.04, 22.64)</td>\n",
       "      <td>å¸‚å–®å®šåŠ ä¸€å¤§å˜…å”å·æ´»ç™¼å·¥ç¨‹å·²ç¶“å®Œæˆå˜…å˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(22.64, 26.88)</td>\n",
       "      <td>è€Œå¸‚å»ºå…±äº¦éƒ½è©±åšŸç·Šæœƒå¼•å…¥å…±åŒç§Ÿä½å–®ä½å˜…å…±å±…æ¨¡å¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.0, 4.32)</td>\n",
       "      <td>å¸Œæœ›åº•æ™‚å‘¢åº¦å‘¢å°±å¯ä»¥è®Šæˆä¸€å€‹å……æ»¿æ–‡åŒ–ç‰¹è‰²åŒåŸ‹æ´»åŠ›å˜…ç¤¾å€</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp                             text\n",
       "0      (0.0, 1.4)                        å…¶å¯¦éƒ½æœ‰ä½¢å˜…åƒ¹å€¼å•¦\n",
       "1     (1.4, 4.68)                    å¯èƒ½æœƒä¿‚åŒèº«ç²‰èªåŒæœ‰é—œä¿‚å•¦\n",
       "2     (4.68, 8.0)             åˆæˆ–è€…å¯èƒ½ä½¢æœƒå¤§å‹•åˆ°ä¸€å€‹åœ°æ–¹å˜…æ–‡åŒ–æ—…æ¸¸å•¦\n",
       "3    (8.0, 15.04)  ä½¢éš•è—ä½åŒåŸ‹ä½¢å°æ–¼å‘¢å€‹ç¤¾æœƒå‰µé€ ç·Šå˜…åƒ¹å€¼å…¶å¯¦éƒ½ä¿‚å¥½é‡è¦å˜…å…ƒç´ åšŸå˜…\n",
       "4  (19.04, 22.64)              å¸‚å–®å®šåŠ ä¸€å¤§å˜…å”å·æ´»ç™¼å·¥ç¨‹å·²ç¶“å®Œæˆå˜…å˜\n",
       "5  (22.64, 26.88)          è€Œå¸‚å»ºå…±äº¦éƒ½è©±åšŸç·Šæœƒå¼•å…¥å…±åŒç§Ÿä½å–®ä½å˜…å…±å±…æ¨¡å¼\n",
       "6     (0.0, 4.32)      å¸Œæœ›åº•æ™‚å‘¢åº¦å‘¢å°±å¯ä»¥è®Šæˆä¸€å€‹å……æ»¿æ–‡åŒ–ç‰¹è‰²åŒåŸ‹æ´»åŠ›å˜…ç¤¾å€"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "df = pd.json_normalize(result, record_path =['chunks'])\n",
    "display(df)\n",
    "\n",
    "# show df in a tablular format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbf9d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
