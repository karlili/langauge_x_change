{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee342d21",
   "metadata": {},
   "source": [
    "the original notebook is from hugging face colab notebook here (https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)\n",
    "\n",
    "\n",
    "Make sure you have the following dependencies installed in your environment\n",
    "\n",
    "```\n",
    "pip install datasets transformers evaulate jiwer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1234f2",
   "metadata": {},
   "source": [
    "the common voice dataset is coming from here (https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/zh-HK/train?p=84)\n",
    "\n",
    "It is part of mozilla foundation common voice project\n",
    "\n",
    "----\n",
    "\n",
    "We could use this format to prepare our own dataset to fine tune our version of whisper\n",
    "\n",
    "----\n",
    "\n",
    "If you want to re-use / avoid to download the voice file every time, you can un-comment the part which specify `cache_dir` and point it to the directory you want those file to be downloaded / already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13205eb410405fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T20:09:39.997469Z",
     "start_time": "2023-11-21T20:02:41.345262Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.17k/8.17k [00:00<00:00, 8.21MB/s]\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.3k/12.3k [00:00<00:00, 14.3MB/s]\n",
      "Downloading extra modules: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.74k/3.74k [00:00<00:00, 25.9MB/s]\n",
      "Downloading extra modules: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.3k/77.3k [00:00<00:00, 1.03MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.6k/14.6k [00:00<00:00, 35.2MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78.7M/78.7M [00:02<00:00, 31.9MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66.4M/66.4M [00:03<00:00, 18.9MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72.0M/72.0M [00:01<00:00, 46.7MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 994M/994M [01:00<00:00, 16.5MB/s]]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 962M/962M [01:16<00:00, 12.5MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 939M/939M [01:31<00:00, 10.2MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 429M/429M [00:30<00:00, 14.3MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.2M/48.2M [00:01<00:00, 26.3MB/s]\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:37<00:00, 55.57s/it] \n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:24<00:00,  4.93s/it]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 745k/745k [00:00<00:00, 2.64MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546k/546k [00:00<00:00, 2.33MB/s]]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 535k/535k [00:00<00:00, 2.23MB/s]]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.8M/30.8M [00:01<00:00, 25.4MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450k/450k [00:00<00:00, 6.02MB/s]]\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  1.27s/it]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 1813.52it/s]\n",
      "Reading metadata...: 3104it [00:00, 188819.72it/s]les/s]\n",
      "Generating train split: 3104 examples [00:00, 8294.39 examples/s]\n",
      "Reading metadata...: 2540it [00:00, 267340.83it/s]examples/s]\n",
      "Generating validation split: 2540 examples [00:00, 9287.60 examples/s]\n",
      "Reading metadata...: 2581it [00:00, 283249.13it/s]es/s]\n",
      "Generating test split: 2581 examples [00:00, 8961.12 examples/s]\n",
      "Reading metadata...: 140241it [00:00, 273918.87it/s]s/s]\n",
      "Generating other split: 140241 examples [00:14, 9640.95 examples/s] \n",
      "Reading metadata...: 1684it [00:00, 135687.41it/s] examples/s]\n",
      "Generating invalidated split: 1684 examples [00:00, 9006.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 5644\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 2581\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append(datasets_dir)\n",
    "\n",
    "# before downloading any new dataset, \n",
    "# make sure to check if it needs to Check and Agrees to the terms first, otherwise the download would fail\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset_name = \"mozilla-foundation/common_voice_16_0\"\n",
    "language_to_train = 'yue'\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"train+validation\",\n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/datasets\"\n",
    "  )\n",
    "\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"test\",  \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/datasets\"\n",
    "  )\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cb70f8c8df1663",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185k/185k [00:00<00:00, 1.30MB/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"tokenizers>=0.14,<0.15\"\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/feature\"\n",
    "  ) # start with the whisper small checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db962245-70f8-4288-9d9d-dc6144d90cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"cantonese\", \n",
    "task=\"transcribe\",\n",
    "cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002a1f20-0b5a-4861-92bb-ee5aceb2f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"cantonese\", \n",
    "task=\"transcribe\",\n",
    "cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/processor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8a338c-6ce8-4b08-866f-dad714ba24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_id': '2ecfe4e00a829397a04e316949bf3058c9ed72b0da9fad2686b0bc3bd98654d8a586e878cc3aa7609bf0359f56e24b3bc0f6f1ec4d1ec958e569bbaaf742560b', 'path': '/Volumes/BACKUP/Coding/HUGGING_FACE/datasets/downloads/extracted/5f8c376b62cbcec81f092e38c43f1519f67645668f8044d9b7c5a51c4297c524/yue_train_0/common_voice_yue_31210647.mp3', 'audio': {'path': '/Volumes/BACKUP/Coding/HUGGING_FACE/datasets/downloads/extracted/5f8c376b62cbcec81f092e38c43f1519f67645668f8044d9b7c5a51c4297c524/yue_train_0/common_voice_yue_31210647.mp3', 'array': array([ 1.45519152e-10,  4.36557457e-11,  4.36557457e-11, ...,\n",
      "       -2.06303957e-06, -1.26592931e-06,  1.36844028e-06]), 'sampling_rate': 16000}, 'sentence': 'ç‡å…§å®¹é•·çŸ­å˜…', 'up_votes': 4, 'down_votes': 0, 'age': 'teens', 'gender': 'male', 'accent': 'é¦™æ¸¯ç²µèª', 'locale': 'yue', 'segment': '', 'variant': ''}\n"
     ]
    }
   ],
   "source": [
    "# Preparing Data\n",
    "\n",
    "# Whisper expecting the audio to be at sampling rate @16000 - this is just to make sure the sampling rate fits whisper's training\n",
    "# Since our input audio is sampled at 48kHz, we need to downsample it to 16kHz prior to passing it to the Whisper feature extractor, \n",
    "# 16kHz being the sampling rate expected by the Whisper model.\n",
    "from datasets import Audio\n",
    "raw_common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(raw_common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd3747",
   "metadata": {},
   "source": [
    "prepare the dataset\n",
    "doing the encoding -> preparing the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e69ad91a-725f-40a6-8dfd-7e3c64af477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5636\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2565\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "finalized_common_voice = raw_common_voice.map(prepare_dataset, \n",
    "  remove_columns=raw_common_voice.column_names[\"train\"], \n",
    "  num_proc=2)\n",
    "print(finalized_common_voice)\n",
    "\n",
    "# Initialize the accelerator\n",
    "# accelerator = Accelerator()\n",
    "\n",
    "# Move the model and dataset to the device\n",
    "# model, common_voice = accelerator.prepare(model, finalized_common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59378d24",
   "metadata": {},
   "source": [
    "the following is the actual training and evaluation of the model\n",
    "\n",
    "using the trainer provided by huggingface\n",
    "\n",
    "Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric. We need to define a compute_metrics function that handles this computation.\n",
    "\n",
    "Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "Define the training configuration: this will be used by the ğŸ¤— Trainer to define the training schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be9c5d3-35df-4995-aa04-eea6fa1b1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b87322a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acfbcd",
   "metadata": {},
   "source": [
    "Evaluation using hugging face metric - WER (Word error rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47506d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (1.26.2)\n",
      "Requirement already satisfied: dill in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.19.4)\n",
      "Requirement already satisfied: packaging in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (23.2)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3bcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da949c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  # cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/models\"\n",
    "  )\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08eb7b",
   "metadata": {},
   "source": [
    "What should be the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a nice youtube video guide / introduction for how to use tensorboard (https://www.youtube.com/watch?v=VJW9wU-1n18&t=4s)\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e8fe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpoppysmic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kenny/Coding/langauge_x_change/whisper/training/wandb/run-20231223_215706-7wxt3qhq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq' target=\"_blank\">sparkling-tree-2</a></strong> to <a href='https://wandb.ai/poppysmic/language-x-change' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/poppysmic/language-x-change' target=\"_blank\">https://wandb.ai/poppysmic/language-x-change</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq' target=\"_blank\">https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x4e4671ee0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"language-x-change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d97c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "import datetime\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H%M\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"model/whisper-small-cantonese_\"+now,  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,  # if we are not using CUDA or non graphics card, use fp16=false\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\",\"wandb\"], #this would requires the tensorboardx to be installed\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=finalized_common_voice[\"train\"],\n",
    "    eval_dataset=finalized_common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    # checkpoint_activations=True\n",
    ")\n",
    "\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e159c",
   "metadata": {},
   "source": [
    "The actual Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc840b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "  5%|â–Œ         | 25/500 [02:42<49:22,  6.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0521, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 50/500 [05:18<46:54,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6707, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 75/500 [07:55<44:45,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.995, 'learning_rate': 1.5e-06, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 100/500 [10:32<41:39,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3957, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 20%|â–ˆâ–ˆ        | 100/500 [31:47<41:39,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25670018792152405, 'eval_wer': 356.92307692307696, 'eval_runtime': 1274.5735, 'eval_samples_per_second': 2.012, 'eval_steps_per_second': 0.252, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 125/500 [34:29<38:57,  6.23s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.193, 'learning_rate': 2.5e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 150/500 [37:03<35:56,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.157, 'learning_rate': 3e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 175/500 [39:40<35:42,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1427, 'learning_rate': 3.5e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 200/500 [42:13<30:51,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1412, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 200/500 [1:01:06<30:51,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17312493920326233, 'eval_wer': 301.8076923076923, 'eval_runtime': 1132.7636, 'eval_samples_per_second': 2.264, 'eval_steps_per_second': 0.283, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 225/500 [1:03:54<28:40,  6.26s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1466, 'learning_rate': 4.5e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 250/500 [1:18:44<2:16:47, 32.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1434, 'learning_rate': 5e-06, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 275/500 [1:36:23<23:24,  6.24s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1218, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 300/500 [1:39:22<41:47, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1358, 'learning_rate': 6e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 300/500 [3:01:50<41:47, 12.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1573217809200287, 'eval_wer': 129.26923076923075, 'eval_runtime': 4947.7065, 'eval_samples_per_second': 0.518, 'eval_steps_per_second': 0.065, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 325/500 [3:04:38<18:58,  6.50s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1332, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 350/500 [3:22:50<17:35,  7.03s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.129, 'learning_rate': 7e-06, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 375/500 [3:41:53<1:55:47, 55.58s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0777, 'learning_rate': 7.500000000000001e-06, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 400/500 [3:44:29<10:12,  6.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0701, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 400/500 [5:22:26<10:12,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1577225923538208, 'eval_wer': 83.96153846153847, 'eval_runtime': 5877.0947, 'eval_samples_per_second': 0.436, 'eval_steps_per_second': 0.055, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 425/500 [6:03:25<2:48:41, 134.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0823, 'learning_rate': 8.5e-06, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 450/500 [6:05:58<05:06,  6.13s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0756, 'learning_rate': 9e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 475/500 [6:08:36<02:41,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0904, 'learning_rate': 9.5e-06, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [6:42:36<00:00, 288.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0822, 'learning_rate': 0.0, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [8:01:09<00:00, 288.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15555481612682343, 'eval_wer': 82.11538461538461, 'eval_runtime': 4711.569, 'eval_samples_per_second': 0.544, 'eval_steps_per_second': 0.068, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [8:01:16<00:00, 57.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 28876.5739, 'train_samples_per_second': 0.277, 'train_steps_per_second': 0.017, 'train_loss': 0.35178972697257993, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.35178972697257993, metrics={'train_runtime': 28876.5739, 'train_samples_per_second': 0.277, 'train_steps_per_second': 0.017, 'train_loss': 0.35178972697257993, 'epoch': 1.42})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d8672e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–‚â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–â–â–‡â–ˆâ–†</td></tr><tr><td>eval/samples_per_second</td><td>â–‡â–ˆâ–â–â–</td></tr><tr><td>eval/steps_per_second</td><td>â–‡â–ˆâ–â–â–</td></tr><tr><td>eval/wer</td><td>â–ˆâ–‡â–‚â–â–</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/learning_rate</td><td>â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–</td></tr><tr><td>train/loss</td><td>â–ˆâ–‡â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/total_flos</td><td>â–</td></tr><tr><td>train/train_loss</td><td>â–</td></tr><tr><td>train/train_runtime</td><td>â–</td></tr><tr><td>train/train_samples_per_second</td><td>â–</td></tr><tr><td>train/train_steps_per_second</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.15555</td></tr><tr><td>eval/runtime</td><td>4711.569</td></tr><tr><td>eval/samples_per_second</td><td>0.544</td></tr><tr><td>eval/steps_per_second</td><td>0.068</td></tr><tr><td>eval/wer</td><td>82.11538</td></tr><tr><td>train/epoch</td><td>1.42</td></tr><tr><td>train/global_step</td><td>500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0822</td></tr><tr><td>train/total_flos</td><td>2.30522017775616e+18</td></tr><tr><td>train/train_loss</td><td>0.35179</td></tr><tr><td>train/train_runtime</td><td>28876.5739</td></tr><tr><td>train/train_samples_per_second</td><td>0.277</td></tr><tr><td>train/train_steps_per_second</td><td>0.017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-tree-2</strong> at: <a href='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq' target=\"_blank\">https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231223_215706-7wxt3qhq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b87e",
   "metadata": {},
   "source": [
    "if you need to push the model to hugging face hub, run the following block\n",
    "\n",
    "```\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is optional. but it would allow you to upload the model to hugging face space later on\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to push\n",
    "\n",
    "#the following arguments are needed only when we are pushing the model to hugging face hub\n",
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: hi, split: test\",\n",
    "    \"language\": \"Cantonese\",\n",
    "    \"model_name\": \"[language-x-change] Custom Whisper for Cantanese\",  # a 'pretty' name for our model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895521a",
   "metadata": {},
   "source": [
    "The following is only needed when we want to deploy a runnable version with our uploaded model on hugging face spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a427339",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"your-own-model\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Hindi\",\n",
    "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531bd73",
   "metadata": {},
   "source": [
    "to use the model we just compiled (https://huggingface.co/docs/transformers/tasks/asr#inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37a6db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - {'text': ' ä¸­ä¸Šç’°å¢ƒåŠç”Ÿä¸€å¸¶é™¤å’—ç•€äººå€‹æ„Ÿè¦ºéˆæ€§ä¹‹å¤–å‘¢åº¦å˜…å»ºç¯‰ç‰©äº¦éƒ½ä¿‚æ–°èˆŠäº¤ç”¨ã—å””è¬›ä½ å””çŸ¥å‘¢åº¦æ›¾ç¶“å‡ºç¾éä¸€å€‹å«åšä¸‰åé–“å˜…ç¤¾å€å•²'}\n",
      "[1] - {'text': 'è¢«ç¨±ç‚ºä¸‰åé–“å˜…ç¤¾å€ï¼Œç›®å‰ä¸¦å†‡å®Œæ•´å˜…æ–‡ç»è¨˜éŒ„è³‡æ–™ï¼Œä¸Šå‚³å–ºåä¹ä¸–ç´€ï¼Œå¸‚å–®é “è¡—åŒåŸ‹å¿…åˆ—è€…äº‹è¡—é™„è¿‘ä¸€å¸¶å˜…è¯äººèšå±…åœ°ï¼Œæ‰€å»ºé€ å˜…ä¸‰åé–“çŸ³å±‹ç§»å¾—å'}\n",
      "[2] - {'text': 'éš¨è‘—ç¤¾æœƒè®Šé· çŸ³å±‹å·²ç¶“ä¸å¾©å­˜åœ¨ç¾æ™‚ ç•ªéŸ‹å…§ä»ç„¶æœ‰æ•¸åº§å¤§ç´„åœ¨1950å¹´ä»£å»ºæˆçš„å”æµå»ºç¯‰åˆ†åˆ¥æ˜¯åœ¨2019å¹´ç¢ºå®šæˆç‚ºäºŒç´šæ­·å²å»ºç¯‰çš„å²ä¸¹é “è¡—88åŠ90è™ŸåŒåŸ‹å¹³ç´šæœ‰å¾…è©•ä¼°çš„è¯é¡æ–¹è¥¿å”æµå»ºç¯‰ç¾¤'}\n",
      "[3] - {'text': ' Kara,å…¶å¯¦ç•¶åˆæ²™æ‹‰é–“å€‹èµ·æºä¿‚'}\n",
      "[4] - {'text': 'å…¶å¯¦å¦‚æœæˆ‘å€‘æ‰¾å›è³‡æ–™çš„è©±æœ€æ—©å…¶å¯¦æˆ‘å€‘æ˜¯åœ¨ä¸€ç™¾å…«é›¶å¹´çš„æ”¿åºœæ†²å ±è¦‹åˆ°ä¸‰åé–“é€™å€‹åå­—å› ç‚ºæˆ‘å€‘ç¾åœ¨åœ¨é¦™æ¸¯çš„åœ°åœ–ä¸Šå…¶å¯¦æˆ‘å€‘éƒ½å¾ˆé›£å¯ä»¥è¦‹åˆ°ä¸‰åé–“é€™å€‹åå­—'}\n",
      "[5] - {'text': 'çœŸä¿‚çŸ¥é“å‘¢å€‹åå˜…äººå‘¢å¤§æ¦‚éƒ½å·²ç¶“å»åˆ°å…­åæ­²æˆ–è€…ä»¥ä¸Šå˜…äººå…ˆè‡³æœƒè­˜å¾—ç”¨å‘¢å€‹å'}\n",
      "[6] - {'text': 'ç•¶æ™‚å…¶å¯¦ä¿‚å‘¢å€‹ä½ç½®å‘¢æ‡‰è©²å°±ä¿‚èµ·å’—å¤§ç´„ä¸‰åé–“'}\n",
      "[7] - {'text': 'å±‹å»ºç¯‰ç¾¤å¦‚æœè‚‰çœ¼è¦‹åˆ°çš„ç—•è·¡å…¶å¯¦å¯èƒ½åªæœ‰è¿”'}\n",
      "[8] - {'text': 'å³ä¿‚å‘¢åº¦ç”Ÿä¸‹é–“è¡—åŠé­šè˜­æœƒå‘¢å€‹æ‹›ç‰Œ'}\n",
      "[9] - {'text': ' We can reflect on the past here, the Saradan area. There used to be a lot of Chinese people coming here.'}\n",
      "[10] - {'text': 'ä½™è˜­æœƒå°±ä¿‚æ¯å¹´ä¸ƒæœˆå˜…æ™‚å€™æœƒèˆ‰è¾¦ä½™è˜­æ€§æœƒå˜…çµ„ç¹”å•¦ä½¢å“‹å…¶å¯¦ä¿‚ä¸€ç­å±…æ°‘'}\n",
      "[11] - {'text': 'çµ„ç¹”å‡ºä¾†å˜…ä¸€å€‹åœ°æ–¹'}\n",
      "[12] - {'text': 'æ–¼å—æ€§æœƒ å°æ–¼ä¸€å€‹è¯äººç¤¾æœƒä¾†èªªéå¸¸é‡è¦è¶…åˆ°ä¸€äº›å­¤é­‚é¬¼ä»¤åˆ°é€™è£¡å¯èƒ½å‚·ç ´ è¡—åŠå¯ä»¥å®‰å¿ƒä¸€é» é€™æ¨£çš„ä¸€å€‹å‚³çµ±ç¿’ä¿—'}\n",
      "[13] - {'text': 'å–ºè¡—é“ä½ˆå±€ä¸Šé¢ä¸‰åé–“ç¤¾å€å˜…ç‰¹è‰²ä¿‚é»å˜…'}\n",
      "[14] - {'text': 'å¦‚æœæƒ³ç†è§£ä¸‰åé–“ç¯„åœå…¶å¯¦æˆ‘å€‘æ‡‰è©²ç”±ä¸‹é¢çš„å£«ä¸¹é “è¡—é–‹å§‹è¨ˆç®—é‚£å…¶å¯¦æ˜¯ä¸€å€‹æ—å¿ƒ'}\n",
      "[15] - {'text': 'è·Ÿä½å°±ä¸€è·¯æ‰“ä¸Šå»åº¦ä¸Šé¢åŠå±±å˜…å …å µç¯„åœ'}\n",
      "[16] - {'text': 'ä¸­é–“è¨˜æ†¶ä¸€å€‹ç¯„åœå…¶å¯¦æˆ‘å€‘éƒ½å¯ä»¥ç†è§£ç‚ºä¸‰åé–“'}\n",
      "[17] - {'text': ' In thisç•¶ä¸­è£¡é¢å‘¢å…¶å¯¦éƒ½æœ‰å””å°‘å˜…åœ°æ–¹å…¨éƒ¨éƒ½ä¿‚ä¸€å•²æ·¨ä¿‚äººè¡Œè»Šå””å…¥å¾—å˜…åœ°æ–¹å•¦'}\n",
      "[18] - {'text': 'åŒ…æ‹¬ä¿‚ä¸€å•²æ–¹åŒåŸ‹ä½ å•¦ï¼Œä¾‹å¦‚æ‡·å¿µæ–¹è¥¿å•¦'}\n",
      "[19] - {'text': 'æˆç‹è¡—äº¦éƒ½ä¿‚æ·¨ä¿‚äººè¡Œå˜…æ¨“æ¢¯åšŸå˜…æœ‰å•²äººäº¦éƒ½æœƒå°‡æ°¸ç†è¡—å‘¢'}\n",
      "[20] - {'text': ' è¨ˆè½å»ä¸‰åé–“å˜…ç¯„åœ'}\n",
      "[21] - {'text': 'æ‰€ä»¥å¯ä»¥è¬›å…¶å¯¦æ²™é˜¿é–“ä¿‚ä¸€å€‹æš´è¡Œå˜…å°å€åšŸ'}\n",
      "[22] - {'text': 'å‘¢å€‹åœ°æ–¹å‘¢å…¶å¯¦å°±ä¿ç•™å’—å¥½å¤šå³ä¿‚å—°å€‹å¹´ä»£äº”åå¹´ä»£è‡³å…­åå¹´ä»£å•¦ä¸‰è‡³å››å±¤é«˜å˜…å ‚æ¨“å»ºç¯‰ç‰©'}\n",
      "[23] - {'text': 'é•·é é»æ¨£åšå…ˆå¯ä»¥ä¿ç•™åˆ°ä¸‰åé–“ç¤¾å€æ­·å²æ•…äº‹'}\n",
      "[24] - {'text': ' The best way to keep your body warm is to use it. So the Yulunanæ€§æœƒ of the 30-storey can practice and make money.'}\n",
      "[25] - {'text': 'å€«è¿‘å¸‚å–®é “è¡—ä¸€å¸¶å˜…å ‚æ¨“ï¼ŒåŸæœ¬è¢«åŠƒå…¥å¸‚å€é‡å»ºå±€å–º 2003å¹´æå‡ºå˜…é‡å»ºè¨ˆåŠƒç•¶ä¸­'}\n",
      "[26] - {'text': 'å…¶å¾Œå› æ‡‰ç¤¾å€äººå£«æå‡ºä¿ç•™å»ºç¯‰ç‰©å˜…è¨´æ±‚ï¼Œä½¿å»ºå±€å–º 2020 å¹´æ”¾æ£„é‡å»ºè¨ˆåŠƒï¼Œç ”ç©¶ä¿è‚²åŒæ´»ç™¼é …ç›®å…¥é¢å˜…å»ºç¯‰ç¾¤'}\n",
      "[27] - {'text': 'é™¤å’—äº‹ä»¶å±€å‘¢å€‹æ´»åŒ–è¨ˆåŠƒï¼Œé™„è¿‘äº¦æœ‰å¤šå€‹æ´»åŒ–æ­·å²å»ºç¯‰å˜…é …ç›®ï¼ŒåŒ…æ‹¬éŒ¢ç”Ÿç‚ºè·é‡Œæ´»åº¦å‰å·²åˆ†è­¦å¯Ÿå®¿èˆå˜…åŸå€‰æ–¹'}\n",
      "[28] - {'text': 'åŠéŒ¢èº«ç‚ºå¿…çƒˆé®å¸‚è¡—å¸‚å ´å˜…é¦™æ¸¯æ–°èåšè¦½é¤¨'}\n",
      "[29] - {'text': ' In the 30-storey area or outside, there are many historical buildings that have been destroyed'}\n",
      "[30] - {'text': 'ä¾‹å¦‚åŒ…æ‹¬æœ‰æ´»åŒ–æˆç‚ºåšè¦½é¤¨åˆæˆ–è€…å¯èƒ½ä¿‚åšå’—ä¸€å•²æ–‡åŒ–æ–‡å‰µå˜…åœ°æ–¹ä½†ä¿‚æˆ‘è‡ªå·±è¦ºå¾—å…¶å¯¦å†‡ä¸€å€‹æ–¹æ³•ä¸€å®šä¿‚ä¸–ç•Œé€šè¡Œæˆ–è€…å°±ä¿‚å«åšä¸€å®šä¿‚æœ€å¥½'}\n",
      "[31] - {'text': 'æœ‰é™£æ™‚å…¶å¯¦æœ‰å°‘å°‘å•†æ¥­å…ƒç´ å–ºè£é¢ï¼Œæœ‰é™£æ™‚éƒ½æœªå¿…ä¿‚ä¸€ä»¶å£äº‹åšŸ'}\n",
      "[32] - {'text': ' Of course, there must be some legal instruments to make sure that the building can be changed as much as possible. Or we should have some better policies to help these buildings sustain.'}\n",
      "[33] - {'text': ' In the cityç™¼å±•å’Œä¿ç•™èˆŠç¤¾å€åŠèˆŠå»ºç¯‰ç‰©æ­·å²ä¹‹é–“å¯ä»¥é»æ¨£æ‹åˆ°å€‹å¹³è¡¡å‘¢?å³æ˜¯åŸå¸‚ç™¼å±•å’Œä¿è‚²å¾ä¾†éƒ½å””æ‡‰è©²ä¿‚å°ç«‹'}\n",
      "[34] - {'text': ' ä¸€å€‹èˆŠå»ºç¯‰ç‰©å…¶å¯¦å°æ–¼ä¸€å€‹ç¤¾å€å…¶å¯¦éƒ½æœ‰ä½¢åƒ¹å€¼å•¦å¯èƒ½æœƒä¿‚'}\n",
      "[35] - {'text': 'åŒèº«ä»½å‹åœ–æœ‰é—œä¿‚åˆæˆ–è€…å¯èƒ½ä½¢æœƒå¸¶å‹•åˆ°ä¸€å€‹åœ°æ–¹å˜…æ–‡åŒ–æ—…éŠ'}\n",
      "[36] - {'text': ' ä½¢æ¾è—ä½åŒåŸ‹ä½¢å°æ–¼å‘¢å€‹ç¤¾æœƒå‰µé€ ç·Šå˜…åƒ¹å€¼å…¶å¯¦éƒ½ä¿‚å¥½é‡è¦å˜…å…ƒç´ åšŸ'}\n",
      "[37] - {'text': ' you'}\n",
      "[38] - {'text': ' The list of the most common sugar-coated hot-blooded public relations has been completed and the city council also said that it will also enter the shared-court-led public housing system. Hopefully, on the other hand, this place can become a smart, unique and lively community.'}\n",
      "[39] - {'text': ' you'}\n",
      "[40] - {'text': ' you'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "def write_contents_to_file(content): \n",
    "    now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    json_object = json.dumps(result, indent=4)\n",
    "    with open('output/'+now+\".json\", \"w\") as f:\n",
    "        f.write(json_object)\n",
    "\n",
    "path = \"model/whisper-small-cantonese_23-12-2023-2157/checkpoint-400\"\n",
    "processor_path = \"model/whisper-small-cantonese_23-12-2023-2157\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "   path, \n",
    "   local_files_only=True,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(processor_path)\n",
    "\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", \n",
    "    model=model,  \n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    # chunk_length_s=5,\n",
    "    max_new_tokens=500,\n",
    "   #  batch_size=16,\n",
    "    # return_timestamps=True\n",
    "   )\n",
    "transcriber.tokenizer.get_decoder_prompt_ids(language='cantonese', task=\"transcribe\")\n",
    "\n",
    "# file_list = [\"Audio1_2.mp3\",\"Audio1_4.mp3\",\"Audio1_5.mp3\",\"Audio1_9.mp3\",\"Audio1_10.mp3\",\"Audio1_11.mp3\"]\n",
    "# for index, file in enumerate(file_list):\n",
    "#     result = transcriber(\"source/\"+file)\n",
    "#     write_contents_to_file(result)\n",
    "#     # also it will print out the result in the following output block\n",
    "#     print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "num_of_chunks = 41\n",
    "file_prefix = \"chunk\";\n",
    "file_suffix = \".mp3\"\n",
    "\n",
    "for index in range(0, num_of_chunks):\n",
    "    result = transcriber(\"source/rthk/\"+file_prefix+str(index)+file_suffix)\n",
    "    # write_contents_to_file(result)\n",
    "    print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb5b8e",
   "metadata": {},
   "source": [
    "install the following dependencies for plotting and tabulation\n",
    "\n",
    "```\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b202c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 1.4)</td>\n",
       "      <td>å…¶å¯¦éƒ½æœ‰ä½¢å˜…åƒ¹å€¼å•¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1.4, 4.68)</td>\n",
       "      <td>å¯èƒ½æœƒä¿‚åŒèº«ç²‰èªåŒæœ‰é—œä¿‚å•¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(4.68, 8.0)</td>\n",
       "      <td>åˆæˆ–è€…å¯èƒ½ä½¢æœƒå¤§å‹•åˆ°ä¸€å€‹åœ°æ–¹å˜…æ–‡åŒ–æ—…æ¸¸å•¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(8.0, 15.04)</td>\n",
       "      <td>ä½¢éš•è—ä½åŒåŸ‹ä½¢å°æ–¼å‘¢å€‹ç¤¾æœƒå‰µé€ ç·Šå˜…åƒ¹å€¼å…¶å¯¦éƒ½ä¿‚å¥½é‡è¦å˜…å…ƒç´ åšŸå˜…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(19.04, 22.64)</td>\n",
       "      <td>å¸‚å–®å®šåŠ ä¸€å¤§å˜…å”å·æ´»ç™¼å·¥ç¨‹å·²ç¶“å®Œæˆå˜…å˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(22.64, 26.88)</td>\n",
       "      <td>è€Œå¸‚å»ºå…±äº¦éƒ½è©±åšŸç·Šæœƒå¼•å…¥å…±åŒç§Ÿä½å–®ä½å˜…å…±å±…æ¨¡å¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.0, 4.32)</td>\n",
       "      <td>å¸Œæœ›åº•æ™‚å‘¢åº¦å‘¢å°±å¯ä»¥è®Šæˆä¸€å€‹å……æ»¿æ–‡åŒ–ç‰¹è‰²åŒåŸ‹æ´»åŠ›å˜…ç¤¾å€</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp                             text\n",
       "0      (0.0, 1.4)                        å…¶å¯¦éƒ½æœ‰ä½¢å˜…åƒ¹å€¼å•¦\n",
       "1     (1.4, 4.68)                    å¯èƒ½æœƒä¿‚åŒèº«ç²‰èªåŒæœ‰é—œä¿‚å•¦\n",
       "2     (4.68, 8.0)             åˆæˆ–è€…å¯èƒ½ä½¢æœƒå¤§å‹•åˆ°ä¸€å€‹åœ°æ–¹å˜…æ–‡åŒ–æ—…æ¸¸å•¦\n",
       "3    (8.0, 15.04)  ä½¢éš•è—ä½åŒåŸ‹ä½¢å°æ–¼å‘¢å€‹ç¤¾æœƒå‰µé€ ç·Šå˜…åƒ¹å€¼å…¶å¯¦éƒ½ä¿‚å¥½é‡è¦å˜…å…ƒç´ åšŸå˜…\n",
       "4  (19.04, 22.64)              å¸‚å–®å®šåŠ ä¸€å¤§å˜…å”å·æ´»ç™¼å·¥ç¨‹å·²ç¶“å®Œæˆå˜…å˜\n",
       "5  (22.64, 26.88)          è€Œå¸‚å»ºå…±äº¦éƒ½è©±åšŸç·Šæœƒå¼•å…¥å…±åŒç§Ÿä½å–®ä½å˜…å…±å±…æ¨¡å¼\n",
       "6     (0.0, 4.32)      å¸Œæœ›åº•æ™‚å‘¢åº¦å‘¢å°±å¯ä»¥è®Šæˆä¸€å€‹å……æ»¿æ–‡åŒ–ç‰¹è‰²åŒåŸ‹æ´»åŠ›å˜…ç¤¾å€"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "df = pd.json_normalize(result, record_path =['chunks'])\n",
    "display(df)\n",
    "\n",
    "# show df in a tablular format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbf9d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
