{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee342d21",
   "metadata": {},
   "source": [
    "the original notebook is from hugging face colab notebook here (https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)\n",
    "\n",
    "\n",
    "Make sure you have the following dependencies installed in your environment\n",
    "\n",
    "```\n",
    "pip install datasets transformers evaulate jiwer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1234f2",
   "metadata": {},
   "source": [
    "the common voice dataset is coming from here (https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/zh-HK/train?p=84)\n",
    "\n",
    "It is part of mozilla foundation common voice project\n",
    "\n",
    "----\n",
    "\n",
    "We could use this format to prepare our own dataset to fine tune our version of whisper\n",
    "\n",
    "----\n",
    "\n",
    "If you want to re-use / avoid to download the voice file every time, you can un-comment the part which specify `cache_dir` and point it to the directory you want those file to be downloaded / already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13205eb410405fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T20:09:39.997469Z",
     "start_time": "2023-11-21T20:02:41.345262Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 8.17k/8.17k [00:00<00:00, 8.21MB/s]\n",
      "Downloading readme: 100%|██████████| 12.3k/12.3k [00:00<00:00, 14.3MB/s]\n",
      "Downloading extra modules: 100%|██████████| 3.74k/3.74k [00:00<00:00, 25.9MB/s]\n",
      "Downloading extra modules: 100%|██████████| 77.3k/77.3k [00:00<00:00, 1.03MB/s]\n",
      "Downloading data: 100%|██████████| 14.6k/14.6k [00:00<00:00, 35.2MB/s]\n",
      "Downloading data: 100%|██████████| 78.7M/78.7M [00:02<00:00, 31.9MB/s]\n",
      "Downloading data: 100%|██████████| 66.4M/66.4M [00:03<00:00, 18.9MB/s]\n",
      "Downloading data: 100%|██████████| 72.0M/72.0M [00:01<00:00, 46.7MB/s]\n",
      "Downloading data: 100%|██████████| 994M/994M [01:00<00:00, 16.5MB/s]]\n",
      "Downloading data: 100%|██████████| 962M/962M [01:16<00:00, 12.5MB/s]\n",
      "Downloading data: 100%|██████████| 939M/939M [01:31<00:00, 10.2MB/s]\n",
      "Downloading data: 100%|██████████| 429M/429M [00:30<00:00, 14.3MB/s]\n",
      "Downloading data: 100%|██████████| 48.2M/48.2M [00:01<00:00, 26.3MB/s]\n",
      "Downloading data files: 100%|██████████| 5/5 [04:37<00:00, 55.57s/it] \n",
      "Extracting data files: 100%|██████████| 5/5 [00:24<00:00,  4.93s/it]\n",
      "Downloading data: 100%|██████████| 745k/745k [00:00<00:00, 2.64MB/s]\n",
      "Downloading data: 100%|██████████| 546k/546k [00:00<00:00, 2.33MB/s]]\n",
      "Downloading data: 100%|██████████| 535k/535k [00:00<00:00, 2.23MB/s]]\n",
      "Downloading data: 100%|██████████| 30.8M/30.8M [00:01<00:00, 25.4MB/s]\n",
      "Downloading data: 100%|██████████| 450k/450k [00:00<00:00, 6.02MB/s]]\n",
      "Downloading data files: 100%|██████████| 5/5 [00:06<00:00,  1.27s/it]\n",
      "Extracting data files: 100%|██████████| 5/5 [00:00<00:00, 1813.52it/s]\n",
      "Reading metadata...: 3104it [00:00, 188819.72it/s]les/s]\n",
      "Generating train split: 3104 examples [00:00, 8294.39 examples/s]\n",
      "Reading metadata...: 2540it [00:00, 267340.83it/s]examples/s]\n",
      "Generating validation split: 2540 examples [00:00, 9287.60 examples/s]\n",
      "Reading metadata...: 2581it [00:00, 283249.13it/s]es/s]\n",
      "Generating test split: 2581 examples [00:00, 8961.12 examples/s]\n",
      "Reading metadata...: 140241it [00:00, 273918.87it/s]s/s]\n",
      "Generating other split: 140241 examples [00:14, 9640.95 examples/s] \n",
      "Reading metadata...: 1684it [00:00, 135687.41it/s] examples/s]\n",
      "Generating invalidated split: 1684 examples [00:00, 9006.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 5644\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 2581\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append(datasets_dir)\n",
    "\n",
    "# before downloading any new dataset, \n",
    "# make sure to check if it needs to Check and Agrees to the terms first, otherwise the download would fail\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset_name = \"mozilla-foundation/common_voice_16_0\"\n",
    "language_to_train = 'yue'\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"train+validation\",\n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/datasets\"\n",
    "  )\n",
    "\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"test\",  \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/datasets\"\n",
    "  )\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cb70f8c8df1663",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessor_config.json: 100%|██████████| 185k/185k [00:00<00:00, 1.30MB/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"tokenizers>=0.14,<0.15\"\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/feature\"\n",
    "  ) # start with the whisper small checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db962245-70f8-4288-9d9d-dc6144d90cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"cantonese\", \n",
    "task=\"transcribe\",\n",
    "cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002a1f20-0b5a-4861-92bb-ee5aceb2f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"cantonese\", \n",
    "task=\"transcribe\",\n",
    "cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/processor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8a338c-6ce8-4b08-866f-dad714ba24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_id': '2ecfe4e00a829397a04e316949bf3058c9ed72b0da9fad2686b0bc3bd98654d8a586e878cc3aa7609bf0359f56e24b3bc0f6f1ec4d1ec958e569bbaaf742560b', 'path': '/Volumes/BACKUP/Coding/HUGGING_FACE/datasets/downloads/extracted/5f8c376b62cbcec81f092e38c43f1519f67645668f8044d9b7c5a51c4297c524/yue_train_0/common_voice_yue_31210647.mp3', 'audio': {'path': '/Volumes/BACKUP/Coding/HUGGING_FACE/datasets/downloads/extracted/5f8c376b62cbcec81f092e38c43f1519f67645668f8044d9b7c5a51c4297c524/yue_train_0/common_voice_yue_31210647.mp3', 'array': array([ 1.45519152e-10,  4.36557457e-11,  4.36557457e-11, ...,\n",
      "       -2.06303957e-06, -1.26592931e-06,  1.36844028e-06]), 'sampling_rate': 16000}, 'sentence': '睇內容長短嘅', 'up_votes': 4, 'down_votes': 0, 'age': 'teens', 'gender': 'male', 'accent': '香港粵語', 'locale': 'yue', 'segment': '', 'variant': ''}\n"
     ]
    }
   ],
   "source": [
    "# Preparing Data\n",
    "\n",
    "# Whisper expecting the audio to be at sampling rate @16000 - this is just to make sure the sampling rate fits whisper's training\n",
    "# Since our input audio is sampled at 48kHz, we need to downsample it to 16kHz prior to passing it to the Whisper feature extractor, \n",
    "# 16kHz being the sampling rate expected by the Whisper model.\n",
    "from datasets import Audio\n",
    "raw_common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(raw_common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd3747",
   "metadata": {},
   "source": [
    "prepare the dataset\n",
    "doing the encoding -> preparing the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e69ad91a-725f-40a6-8dfd-7e3c64af477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5636\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2565\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "finalized_common_voice = raw_common_voice.map(prepare_dataset, \n",
    "  remove_columns=raw_common_voice.column_names[\"train\"], \n",
    "  num_proc=2)\n",
    "print(finalized_common_voice)\n",
    "\n",
    "# Initialize the accelerator\n",
    "# accelerator = Accelerator()\n",
    "\n",
    "# Move the model and dataset to the device\n",
    "# model, common_voice = accelerator.prepare(model, finalized_common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59378d24",
   "metadata": {},
   "source": [
    "the following is the actual training and evaluation of the model\n",
    "\n",
    "using the trainer provided by huggingface\n",
    "\n",
    "Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric. We need to define a compute_metrics function that handles this computation.\n",
    "\n",
    "Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "Define the training configuration: this will be used by the 🤗 Trainer to define the training schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be9c5d3-35df-4995-aa04-eea6fa1b1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b87322a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acfbcd",
   "metadata": {},
   "source": [
    "Evaluation using hugging face metric - WER (Word error rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47506d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (1.26.2)\n",
      "Requirement already satisfied: dill in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.19.4)\n",
      "Requirement already satisfied: packaging in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (23.2)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3bcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da949c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  # cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/models\"\n",
    "  )\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08eb7b",
   "metadata": {},
   "source": [
    "What should be the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a nice youtube video guide / introduction for how to use tensorboard (https://www.youtube.com/watch?v=VJW9wU-1n18&t=4s)\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e8fe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpoppysmic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kenny/Coding/langauge_x_change/whisper/training/wandb/run-20231223_215706-7wxt3qhq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq' target=\"_blank\">sparkling-tree-2</a></strong> to <a href='https://wandb.ai/poppysmic/language-x-change' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/poppysmic/language-x-change' target=\"_blank\">https://wandb.ai/poppysmic/language-x-change</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq' target=\"_blank\">https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x4e4671ee0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"language-x-change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d97c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "import datetime\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H%M\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"model/whisper-small-cantonese_\"+now,  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,  # if we are not using CUDA or non graphics card, use fp16=false\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\",\"wandb\"], #this would requires the tensorboardx to be installed\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=finalized_common_voice[\"train\"],\n",
    "    eval_dataset=finalized_common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    # checkpoint_activations=True\n",
    ")\n",
    "\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e159c",
   "metadata": {},
   "source": [
    "The actual Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc840b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "  5%|▌         | 25/500 [02:42<49:22,  6.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0521, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 50/500 [05:18<46:54,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6707, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 75/500 [07:55<44:45,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.995, 'learning_rate': 1.5e-06, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [10:32<41:39,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3957, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 20%|██        | 100/500 [31:47<41:39,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25670018792152405, 'eval_wer': 356.92307692307696, 'eval_runtime': 1274.5735, 'eval_samples_per_second': 2.012, 'eval_steps_per_second': 0.252, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 125/500 [34:29<38:57,  6.23s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.193, 'learning_rate': 2.5e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 150/500 [37:03<35:56,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.157, 'learning_rate': 3e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 175/500 [39:40<35:42,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1427, 'learning_rate': 3.5e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [42:13<30:51,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1412, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|████      | 200/500 [1:01:06<30:51,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17312493920326233, 'eval_wer': 301.8076923076923, 'eval_runtime': 1132.7636, 'eval_samples_per_second': 2.264, 'eval_steps_per_second': 0.283, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 225/500 [1:03:54<28:40,  6.26s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1466, 'learning_rate': 4.5e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 250/500 [1:18:44<2:16:47, 32.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1434, 'learning_rate': 5e-06, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 275/500 [1:36:23<23:24,  6.24s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1218, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [1:39:22<41:47, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1358, 'learning_rate': 6e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 300/500 [3:01:50<41:47, 12.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1573217809200287, 'eval_wer': 129.26923076923075, 'eval_runtime': 4947.7065, 'eval_samples_per_second': 0.518, 'eval_steps_per_second': 0.065, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 325/500 [3:04:38<18:58,  6.50s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1332, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 350/500 [3:22:50<17:35,  7.03s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.129, 'learning_rate': 7e-06, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 375/500 [3:41:53<1:55:47, 55.58s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0777, 'learning_rate': 7.500000000000001e-06, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 400/500 [3:44:29<10:12,  6.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0701, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 400/500 [5:22:26<10:12,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1577225923538208, 'eval_wer': 83.96153846153847, 'eval_runtime': 5877.0947, 'eval_samples_per_second': 0.436, 'eval_steps_per_second': 0.055, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 85%|████████▌ | 425/500 [6:03:25<2:48:41, 134.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0823, 'learning_rate': 8.5e-06, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 450/500 [6:05:58<05:06,  6.13s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0756, 'learning_rate': 9e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 475/500 [6:08:36<02:41,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0904, 'learning_rate': 9.5e-06, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [6:42:36<00:00, 288.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0822, 'learning_rate': 0.0, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "100%|██████████| 500/500 [8:01:09<00:00, 288.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15555481612682343, 'eval_wer': 82.11538461538461, 'eval_runtime': 4711.569, 'eval_samples_per_second': 0.544, 'eval_steps_per_second': 0.068, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n",
      "100%|██████████| 500/500 [8:01:16<00:00, 57.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 28876.5739, 'train_samples_per_second': 0.277, 'train_steps_per_second': 0.017, 'train_loss': 0.35178972697257993, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.35178972697257993, metrics={'train_runtime': 28876.5739, 'train_samples_per_second': 0.277, 'train_steps_per_second': 0.017, 'train_loss': 0.35178972697257993, 'epoch': 1.42})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d8672e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▇█▆</td></tr><tr><td>eval/samples_per_second</td><td>▇█▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▇█▁▁▁</td></tr><tr><td>eval/wer</td><td>█▇▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██▁</td></tr><tr><td>train/loss</td><td>█▇▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.15555</td></tr><tr><td>eval/runtime</td><td>4711.569</td></tr><tr><td>eval/samples_per_second</td><td>0.544</td></tr><tr><td>eval/steps_per_second</td><td>0.068</td></tr><tr><td>eval/wer</td><td>82.11538</td></tr><tr><td>train/epoch</td><td>1.42</td></tr><tr><td>train/global_step</td><td>500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0822</td></tr><tr><td>train/total_flos</td><td>2.30522017775616e+18</td></tr><tr><td>train/train_loss</td><td>0.35179</td></tr><tr><td>train/train_runtime</td><td>28876.5739</td></tr><tr><td>train/train_samples_per_second</td><td>0.277</td></tr><tr><td>train/train_steps_per_second</td><td>0.017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-tree-2</strong> at: <a href='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq' target=\"_blank\">https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231223_215706-7wxt3qhq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b87e",
   "metadata": {},
   "source": [
    "if you need to push the model to hugging face hub, run the following block\n",
    "\n",
    "```\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is optional. but it would allow you to upload the model to hugging face space later on\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to push\n",
    "\n",
    "#the following arguments are needed only when we are pushing the model to hugging face hub\n",
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: hi, split: test\",\n",
    "    \"language\": \"Cantonese\",\n",
    "    \"model_name\": \"[language-x-change] Custom Whisper for Cantanese\",  # a 'pretty' name for our model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895521a",
   "metadata": {},
   "source": [
    "The following is only needed when we want to deploy a runnable version with our uploaded model on hugging face spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a427339",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"your-own-model\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Hindi\",\n",
    "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531bd73",
   "metadata": {},
   "source": [
    "to use the model we just compiled (https://huggingface.co/docs/transformers/tasks/asr#inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37a6db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - {'text': ' 中上環境半生一帶除咗畀人個感覺靈性之外呢度嘅建築物亦都係新舊交用㗎唔講你唔知呢度曾經出現過一個叫做三十間嘅社區啲'}\n",
      "[1] - {'text': '被稱為三十間嘅社區，目前並冇完整嘅文獻記錄資料，上傳喺十九世紀，市單頓街同埋必列者事街附近一帶嘅華人聚居地，所建造嘅三十間石屋移得名'}\n",
      "[2] - {'text': '隨著社會變遷 石屋已經不復存在現時 番韋內仍然有數座大約在1950年代建成的唐流建築分別是在2019年確定成為二級歷史建築的史丹頓街88及90號同埋平級有待評估的華顏方西唐流建築群'}\n",
      "[3] - {'text': ' Kara,其實當初沙拉間個起源係'}\n",
      "[4] - {'text': '其實如果我們找回資料的話最早其實我們是在一百八零年的政府憲報見到三十間這個名字因為我們現在在香港的地圖上其實我們都很難可以見到三十間這個名字'}\n",
      "[5] - {'text': '真係知道呢個名嘅人呢大概都已經去到六十歲或者以上嘅人先至會識得用呢個名'}\n",
      "[6] - {'text': '當時其實係呢個位置呢應該就係起咗大約三十間'}\n",
      "[7] - {'text': '屋建築群如果肉眼見到的痕跡其實可能只有返'}\n",
      "[8] - {'text': '即係呢度生下間街坊魚蘭會呢個招牌'}\n",
      "[9] - {'text': ' We can reflect on the past here, the Saradan area. There used to be a lot of Chinese people coming here.'}\n",
      "[10] - {'text': '余蘭會就係每年七月嘅時候會舉辦余蘭性會嘅組織啦佢哋其實係一班居民'}\n",
      "[11] - {'text': '組織出來嘅一個地方'}\n",
      "[12] - {'text': '於南性會 對於一個華人社會來說非常重要超到一些孤魂鬼令到這裡可能傷破 街坊可以安心一點 這樣的一個傳統習俗'}\n",
      "[13] - {'text': '喺街道佈局上面三十間社區嘅特色係點嘅'}\n",
      "[14] - {'text': '如果想理解三十間範圍其實我們應該由下面的士丹頓街開始計算那其實是一個族心'}\n",
      "[15] - {'text': '跟住就一路打上去度上面半山嘅堅堵範圍'}\n",
      "[16] - {'text': '中間記憶一個範圍其實我們都可以理解為三十間'}\n",
      "[17] - {'text': ' In this當中裡面呢其實都有唔少嘅地方全部都係一啲淨係人行車唔入得嘅地方啦'}\n",
      "[18] - {'text': '包括係一啲方同埋你啦，例如懷念方西啦'}\n",
      "[19] - {'text': '成王街亦都係淨係人行嘅樓梯嚟嘅有啲人亦都會將永理街呢'}\n",
      "[20] - {'text': ' 計落去三十間嘅範圍'}\n",
      "[21] - {'text': '所以可以講其實沙阿間係一個暴行嘅小區嚟'}\n",
      "[22] - {'text': '呢個地方呢其實就保留咗好多即係嗰個年代五十年代至六十年代啦三至四層高嘅堂樓建築物'}\n",
      "[23] - {'text': '長遠點樣做先可以保留到三十間社區歷史故事'}\n",
      "[24] - {'text': ' The best way to keep your body warm is to use it. So the Yulunan性會 of the 30-storey can practice and make money.'}\n",
      "[25] - {'text': '倫近市單頓街一帶嘅堂樓，原本被劃入市區重建局喺 2003年提出嘅重建計劃當中'}\n",
      "[26] - {'text': '其後因應社區人士提出保留建築物嘅訴求，使建局喺 2020 年放棄重建計劃，研究保育同活發項目入面嘅建築群'}\n",
      "[27] - {'text': '除咗事件局呢個活化計劃，附近亦有多個活化歷史建築嘅項目，包括錢生為荷里活度前已分警察宿舍嘅原倉方'}\n",
      "[28] - {'text': '及錢身為必烈遮市街市場嘅香港新聞博覽館'}\n",
      "[29] - {'text': ' In the 30-storey area or outside, there are many historical buildings that have been destroyed'}\n",
      "[30] - {'text': '例如包括有活化成為博覽館又或者可能係做咗一啲文化文創嘅地方但係我自己覺得其實冇一個方法一定係世界通行或者就係叫做一定係最好'}\n",
      "[31] - {'text': '有陣時其實有少少商業元素喺裏面，有陣時都未必係一件壞事嚟'}\n",
      "[32] - {'text': ' Of course, there must be some legal instruments to make sure that the building can be changed as much as possible. Or we should have some better policies to help these buildings sustain.'}\n",
      "[33] - {'text': ' In the city發展和保留舊社區及舊建築物歷史之間可以點樣拎到個平衡呢?即是城市發展和保育從來都唔應該係對立'}\n",
      "[34] - {'text': ' 一個舊建築物其實對於一個社區其實都有佢價值啦可能會係'}\n",
      "[35] - {'text': '同身份型圖有關係又或者可能佢會帶動到一個地方嘅文化旅遊'}\n",
      "[36] - {'text': ' 佢揾藏住同埋佢對於呢個社會創造緊嘅價值其實都係好重要嘅元素嚟'}\n",
      "[37] - {'text': ' you'}\n",
      "[38] - {'text': ' The list of the most common sugar-coated hot-blooded public relations has been completed and the city council also said that it will also enter the shared-court-led public housing system. Hopefully, on the other hand, this place can become a smart, unique and lively community.'}\n",
      "[39] - {'text': ' you'}\n",
      "[40] - {'text': ' you'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "def write_contents_to_file(content): \n",
    "    now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    json_object = json.dumps(result, indent=4)\n",
    "    with open('output/'+now+\".json\", \"w\") as f:\n",
    "        f.write(json_object)\n",
    "\n",
    "path = \"model/whisper-small-cantonese_23-12-2023-2157/checkpoint-400\"\n",
    "processor_path = \"model/whisper-small-cantonese_23-12-2023-2157\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "   path, \n",
    "   local_files_only=True,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(processor_path)\n",
    "\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", \n",
    "    model=model,  \n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    # chunk_length_s=5,\n",
    "    max_new_tokens=500,\n",
    "   #  batch_size=16,\n",
    "    # return_timestamps=True\n",
    "   )\n",
    "transcriber.tokenizer.get_decoder_prompt_ids(language='cantonese', task=\"transcribe\")\n",
    "\n",
    "# file_list = [\"Audio1_2.mp3\",\"Audio1_4.mp3\",\"Audio1_5.mp3\",\"Audio1_9.mp3\",\"Audio1_10.mp3\",\"Audio1_11.mp3\"]\n",
    "# for index, file in enumerate(file_list):\n",
    "#     result = transcriber(\"source/\"+file)\n",
    "#     write_contents_to_file(result)\n",
    "#     # also it will print out the result in the following output block\n",
    "#     print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "num_of_chunks = 41\n",
    "file_prefix = \"chunk\";\n",
    "file_suffix = \".mp3\"\n",
    "\n",
    "for index in range(0, num_of_chunks):\n",
    "    result = transcriber(\"source/rthk/\"+file_prefix+str(index)+file_suffix)\n",
    "    # write_contents_to_file(result)\n",
    "    print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb5b8e",
   "metadata": {},
   "source": [
    "install the following dependencies for plotting and tabulation\n",
    "\n",
    "```\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b202c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 1.4)</td>\n",
       "      <td>其實都有佢嘅價值啦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1.4, 4.68)</td>\n",
       "      <td>可能會係同身粉認同有關係啦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(4.68, 8.0)</td>\n",
       "      <td>又或者可能佢會大動到一個地方嘅文化旅游啦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(8.0, 15.04)</td>\n",
       "      <td>佢隕藏住同埋佢對於呢個社會創造緊嘅價值其實都係好重要嘅元素嚟嘅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(19.04, 22.64)</td>\n",
       "      <td>市單定加一大嘅唐州活發工程已經完成嘅嘞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(22.64, 26.88)</td>\n",
       "      <td>而市建共亦都話嚟緊會引入共同租住單位嘅共居模式</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.0, 4.32)</td>\n",
       "      <td>希望底時呢度呢就可以變成一個充滿文化特色同埋活力嘅社區</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp                             text\n",
       "0      (0.0, 1.4)                        其實都有佢嘅價值啦\n",
       "1     (1.4, 4.68)                    可能會係同身粉認同有關係啦\n",
       "2     (4.68, 8.0)             又或者可能佢會大動到一個地方嘅文化旅游啦\n",
       "3    (8.0, 15.04)  佢隕藏住同埋佢對於呢個社會創造緊嘅價值其實都係好重要嘅元素嚟嘅\n",
       "4  (19.04, 22.64)              市單定加一大嘅唐州活發工程已經完成嘅嘞\n",
       "5  (22.64, 26.88)          而市建共亦都話嚟緊會引入共同租住單位嘅共居模式\n",
       "6     (0.0, 4.32)      希望底時呢度呢就可以變成一個充滿文化特色同埋活力嘅社區"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "df = pd.json_normalize(result, record_path =['chunks'])\n",
    "display(df)\n",
    "\n",
    "# show df in a tablular format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbf9d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
