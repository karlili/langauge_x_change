{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee342d21",
   "metadata": {},
   "source": [
    "the original notebook is from hugging face colab notebook here (https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)\n",
    "\n",
    "\n",
    "Make sure you have the following dependencies installed in your environment\n",
    "\n",
    "```\n",
    "pip install datasets transformers evaulate jiwer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1234f2",
   "metadata": {},
   "source": [
    "the common voice dataset is coming from here (https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/zh-HK/train?p=84)\n",
    "\n",
    "It is part of mozilla foundation common voice project\n",
    "\n",
    "----\n",
    "\n",
    "We could use this format to prepare our own dataset to fine tune our version of whisper\n",
    "\n",
    "----\n",
    "\n",
    "If you want to re-use / avoid to download the voice file every time, you can un-comment the part which specify `cache_dir` and point it to the directory you want those file to be downloaded / already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13205eb410405fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T20:09:39.997469Z",
     "start_time": "2023-11-21T20:02:41.345262Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 5636\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 2565\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append(datasets_dir)\n",
    "\n",
    "# before downloading any new dataset, \n",
    "# make sure to check if it needs to Check and Agrees to the terms first, otherwise the download would fail\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset_name = \"mozilla-foundation/common_voice_15_0\"\n",
    "language_to_train = 'yue'\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"train+validation\",\n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/datasets\"\n",
    "  )\n",
    "\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"test\",  \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/datasets\"\n",
    "  )\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93cb70f8c8df1663",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip install \"tokenizers>=0.14,<0.15\"\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/feature\"\n",
    "  ) # start with the whisper small checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db962245-70f8-4288-9d9d-dc6144d90cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"cantonese\", \n",
    "task=\"transcribe\",\n",
    "cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "002a1f20-0b5a-4861-92bb-ee5aceb2f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"cantonese\", \n",
    "task=\"transcribe\",\n",
    "cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/processor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d8a338c-6ce8-4b08-866f-dad714ba24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_id': '2ecfe4e00a829397a04e316949bf3058c9ed72b0da9fad2686b0bc3bd98654d8a586e878cc3aa7609bf0359f56e24b3bc0f6f1ec4d1ec958e569bbaaf742560b', 'path': '/Volumes/BACKUP/Coding/HUGGING_FACE/datasets/downloads/extracted/5f8c376b62cbcec81f092e38c43f1519f67645668f8044d9b7c5a51c4297c524/yue_train_0/common_voice_yue_31210647.mp3', 'audio': {'path': '/Volumes/BACKUP/Coding/HUGGING_FACE/datasets/downloads/extracted/5f8c376b62cbcec81f092e38c43f1519f67645668f8044d9b7c5a51c4297c524/yue_train_0/common_voice_yue_31210647.mp3', 'array': array([ 1.45519152e-10,  4.36557457e-11,  4.36557457e-11, ...,\n",
      "       -2.06303957e-06, -1.26592931e-06,  1.36844028e-06]), 'sampling_rate': 16000}, 'sentence': '睇內容長短嘅', 'up_votes': 4, 'down_votes': 0, 'age': 'teens', 'gender': 'male', 'accent': '香港粵語', 'locale': 'yue', 'segment': '', 'variant': ''}\n"
     ]
    }
   ],
   "source": [
    "# Preparing Data\n",
    "\n",
    "# Whisper expecting the audio to be at sampling rate @16000 - this is just to make sure the sampling rate fits whisper's training\n",
    "# Since our input audio is sampled at 48kHz, we need to downsample it to 16kHz prior to passing it to the Whisper feature extractor, \n",
    "# 16kHz being the sampling rate expected by the Whisper model.\n",
    "from datasets import Audio\n",
    "raw_common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(raw_common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd3747",
   "metadata": {},
   "source": [
    "prepare the dataset\n",
    "doing the encoding -> preparing the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e69ad91a-725f-40a6-8dfd-7e3c64af477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 5636/5636 [08:57<00:00, 10.49 examples/s]\n",
      "Map (num_proc=2): 100%|██████████| 2565/2565 [04:05<00:00, 10.46 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5636\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2565\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "finalized_common_voice = raw_common_voice.map(prepare_dataset, \n",
    "  remove_columns=raw_common_voice.column_names[\"train\"], \n",
    "  num_proc=2)\n",
    "print(finalized_common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59378d24",
   "metadata": {},
   "source": [
    "the following is the actual training and evaluation of the model\n",
    "\n",
    "using the trainer provided by huggingface\n",
    "\n",
    "Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric. We need to define a compute_metrics function that handles this computation.\n",
    "\n",
    "Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "Define the training configuration: this will be used by the 🤗 Trainer to define the training schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2be9c5d3-35df-4995-aa04-eea6fa1b1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b87322a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acfbcd",
   "metadata": {},
   "source": [
    "Evaluation using hugging face metric - WER (Word error rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47506d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (1.26.2)\n",
      "Requirement already satisfied: dill in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (0.19.4)\n",
      "Requirement already satisfied: packaging in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from evaluate) (23.2)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f3bcbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.49k/4.49k [00:00<00:00, 12.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1da949c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/models\"\n",
    "  )\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08eb7b",
   "metadata": {},
   "source": [
    "What should be the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a nice youtube video guide / introduction for how to use tensorboard (https://www.youtube.com/watch?v=VJW9wU-1n18&t=4s)\n",
    "!pip install tensorboardx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d97c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H%M\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"model/whisper-small-cantonese_\"+now,  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,  # if we are not using CUDA or non graphics card, use fp16=false\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"], #this would requires the tensorboardx to be installed\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=finalized_common_voice[\"train\"],\n",
    "    eval_dataset=finalized_common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    # checkpoint_activations=True\n",
    ")\n",
    "\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e159c",
   "metadata": {},
   "source": [
    "The actual Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0bc840b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 8:06:49, Epoch 11/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.157449</td>\n",
       "      <td>69.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.171989</td>\n",
       "      <td>63.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.181519</td>\n",
       "      <td>61.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.186846</td>\n",
       "      <td>61.307692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.05433571213460527, metrics={'train_runtime': 29224.9012, 'train_samples_per_second': 2.19, 'train_steps_per_second': 0.137, 'total_flos': 1.843137234763776e+19, 'train_loss': 0.05433571213460527, 'epoch': 11.33})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b87e",
   "metadata": {},
   "source": [
    "if you need to push the model to hugging face hub, run the following block\n",
    "\n",
    "```\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is optional. but it would allow you to upload the model to hugging face space later on\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to push\n",
    "\n",
    "#the following arguments are needed only when we are pushing the model to hugging face hub\n",
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: hi, split: test\",\n",
    "    \"language\": \"Cantonese\",\n",
    "    \"model_name\": \"[language-x-change] Custom Whisper for Cantanese\",  # a 'pretty' name for our model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895521a",
   "metadata": {},
   "source": [
    "The following is only needed when we want to deploy a runnable version with our uploaded model on hugging face spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a427339",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"your-own-model\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Hindi\",\n",
    "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531bd73",
   "metadata": {},
   "source": [
    "to use the model we just compiled (https://huggingface.co/docs/transformers/tasks/asr#inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37a6db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - {'text': ' 中上環近半山一帶 除咗畀人個感覺靈性之外 呢度嘅建築物亦都係新夠交融㗎 唔講你唔知 呢度曾經出現嗰個叫做三日間嘅社區啲'}\n",
      "[1] - {'text': ' 被稱為沙亞間嘅社區 目前並冇完整嘅文獻記錄資料 上傳喺十九世紀 士丹頓街同埋必烈者時間附近一帶嘅華人聚居地 所建造嘅三十間石屋而得名'}\n",
      "[2] - {'text': '隨住社會變遷 石屋已經不復存在現時 方位內仍有數座 大約喺一九五零年代建成嘅唐流建築分別係喺二零一九年 確定成為二級歷史建築嘅史丹頓街八十八及九十號同埋平級有代蘋果嘅話言方西唐流建築群'}\n",
      "[3] - {'text': ' Carol,其實當初三十間個起源係'}\n",
      "[4] - {'text': '其實如果揾返資料的話 最早其實我哋係喺一百八零年嘅政府憲報度見到三十間呢個名嘅 因為我哋而家喺香港嘅地圖上面 其實我哋都好難可以見到三十間呢個名嘅喇 已經'}\n",
      "[5] - {'text': '真係知道呢個名嘅人呢 大概都已經去到六十歲或者以上嘅人先至會識得用呢個名'}\n",
      "[6] - {'text': '當時其實係呢個位置應該就係起咗大約三十間'}\n",
      "[7] - {'text': '如果肉眼見到嘅痕跡其實可能只係得返'}\n",
      "[8] - {'text': '即係呢一度三十間街坊儒蘭會呢個招牌'}\n",
      "[9] - {'text': '可能就係唯一我哋可以反映到以前呢度真係叫到沙亞間嘅一樣嘢咁上環以前其實有好多華人最高嘅地方嚟'}\n",
      "[10] - {'text': '佢哋其實係一班居民'}\n",
      "[11] - {'text': '組織出來嘅一個地方嚟'}\n",
      "[12] - {'text': '儒蘭性會 其實對於一個華人社會來講係非常之重要啦超到一啲孤魂嘅鬼 令到呢度嘅可能傷破 街坊可以安心啲 咁樣嘅一個傳統習俗啦'}\n",
      "[13] - {'text': '喺街道佈局上面三十間社區嘅特色係點嘅'}\n",
      "[14] - {'text': '其實如果想理解三十間嘅範圍其實我哋應該由下面嘅時單頓街開始計啦嗰個其實係個俗心'}\n",
      "[15] - {'text': '跟住就一路打上去到上面半山嘅堅到範圍'}\n",
      "[16] - {'text': '中間記憶一個範圍其實我哋都可以理解為三十間'}\n",
      "[17] - {'text': '在呢度當中裏面其實都有唔少嘅地方 全部都係一啲淨係人行 車唔入得嘅地方啦'}\n",
      "[18] - {'text': '包括係一啲方同埋你啦例如懷念方西啦'}\n",
      "[19] - {'text': '成王街亦都係淨係人行嘅樓梯嚟嘅有啲人亦都會將永理街呢'}\n",
      "[20] - {'text': '計落去三十間嘅範圍'}\n",
      "[21] - {'text': '所以可以講其實沙奸係一個步行嘅小區嚟'}\n",
      "[22] - {'text': '呢個地方呢其實就保留咗好多即係嗰個年代五十年代至六十年代啦三至四層高嘅塘樓建築物'}\n",
      "[23] - {'text': '今場遠點樣做先可以保留到三十間社區嘅歷史故事啊'}\n",
      "[24] - {'text': '保育其實最好嘅方法就係活用落去啦。咁所以呢其實三十間嘅餘難性會呢如果可以繼續一路practice落去繼續實踐落去嘅話呢就會係最好嘅保育方法'}\n",
      "[25] - {'text': '近近市單頓街一帶嘅塘柳原本被劃入市區重建局喺二零零三年提出嘅重建計劃當中'}\n",
      "[26] - {'text': '其後因應社區人士提出保留建築物嘅訴求，使建局喺二零二零年放棄重建計劃，研究保育同活發項目入面嘅建築群'}\n",
      "[27] - {'text': '除咗事件局呢個活化計劃附近亦有多個活化歷史建築嘅項目包括前身為荷里活度前以分警察宿舍嘅原倉方'}\n",
      "[28] - {'text': '及前身為必烈遮市街市場嘅香港新聞博覽館'}\n",
      "[29] - {'text': '三十間嘅範圍內或外面其實都有唔少嘅歷史建築物其實都已經活化緊㗎喇'}\n",
      "[30] - {'text': '例如包括有活化成為博覽館或者可能係做咗一啲文化文創嘅地方但係我自己覺得其實冇一個方法一定係世界通行或者係就係叫做一定係最好'}\n",
      "[31] - {'text': '有時其實有少少商業元素在裡面有時都未必係一件壞事來'}\n",
      "[32] - {'text': '當然啦一定要有一啲啫法例去約束住嗰個建築物其實嗰個改動可以有幾多啦噉又或者可能我哋應該都要有一啲好啲嘅政策嘅配套去幫助呢啲建築物繼續去sustain我'}\n",
      "[33] - {'text': ' In the history between the city發展及保留舊社區及舊建築物之間可以點樣拎到個平衡呢?即係城市發展及保育從來都唔應該係對立'}\n",
      "[34] - {'text': ' 一個舊建築物其實對於一個社區其實都有佢嘅價值啦 可能會係'}\n",
      "[35] - {'text': '同身份認圖有關係啦又或者可能佢會大動到一個地方嘅文化旅遊啦'}\n",
      "[36] - {'text': ' 佢揾藏住同埋佢對於呢個社會 創造緊嘅價值其實呢都係好重要嘅元素嚟'}\n",
      "[37] - {'text': ' Music'}\n",
      "[38] - {'text': ' The list of single-use units with a large pond-hole processing process has been completed and the construction of the city also said that it will lead to the shared-court-downloading of the landline. Hopefully, on the other hand, this place can become a smart, culture-specific and active community'}\n",
      "[39] - {'text': ' you'}\n",
      "[40] - {'text': ' BING'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "def write_contents_to_file(content): \n",
    "    now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    json_object = json.dumps(result, indent=4)\n",
    "    with open('output/'+now+\".json\", \"w\") as f:\n",
    "        f.write(json_object)\n",
    "\n",
    "path = \"model/whisper-small-cantonese_18-12-2023-22-27/checkpoint-4000\"\n",
    "processor_path = \"model/whisper-small-cantonese_18-12-2023-22-27\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "   path, \n",
    "   local_files_only=True,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(processor_path)\n",
    "\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", \n",
    "    model=model,  \n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    # chunk_length_s=20,\n",
    "    max_new_tokens=500,\n",
    "   #  batch_size=16,\n",
    "    # return_timestamps=True\n",
    "   )\n",
    "transcriber.tokenizer.get_decoder_prompt_ids(language='cantonese', task=\"transcribe\")\n",
    "\n",
    "# file_list = [\"Audio1_2.mp3\",\"Audio1_4.mp3\",\"Audio1_5.mp3\",\"Audio1_9.mp3\",\"Audio1_10.mp3\",\"Audio1_11.mp3\"]\n",
    "# for index, file in enumerate(file_list):\n",
    "#     result = transcriber(\"source/\"+file)\n",
    "#     write_contents_to_file(result)\n",
    "#     # also it will print out the result in the following output block\n",
    "#     print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "num_of_chunks = 41\n",
    "file_prefix = \"chunk\";\n",
    "file_suffix = \".mp3\"\n",
    "\n",
    "for index in range(0, num_of_chunks):\n",
    "    result = transcriber(\"source/rthk/\"+file_prefix+str(index)+file_suffix)\n",
    "    # write_contents_to_file(result)\n",
    "    print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb5b8e",
   "metadata": {},
   "source": [
    "install the following dependencies for plotting and tabulation\n",
    "\n",
    "```\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b202c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 1.4)</td>\n",
       "      <td>其實都有佢嘅價值啦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1.4, 4.68)</td>\n",
       "      <td>可能會係同身粉認同有關係啦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(4.68, 8.0)</td>\n",
       "      <td>又或者可能佢會大動到一個地方嘅文化旅游啦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(8.0, 15.04)</td>\n",
       "      <td>佢隕藏住同埋佢對於呢個社會創造緊嘅價值其實都係好重要嘅元素嚟嘅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(19.04, 22.64)</td>\n",
       "      <td>市單定加一大嘅唐州活發工程已經完成嘅嘞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(22.64, 26.88)</td>\n",
       "      <td>而市建共亦都話嚟緊會引入共同租住單位嘅共居模式</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.0, 4.32)</td>\n",
       "      <td>希望底時呢度呢就可以變成一個充滿文化特色同埋活力嘅社區</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp                             text\n",
       "0      (0.0, 1.4)                        其實都有佢嘅價值啦\n",
       "1     (1.4, 4.68)                    可能會係同身粉認同有關係啦\n",
       "2     (4.68, 8.0)             又或者可能佢會大動到一個地方嘅文化旅游啦\n",
       "3    (8.0, 15.04)  佢隕藏住同埋佢對於呢個社會創造緊嘅價值其實都係好重要嘅元素嚟嘅\n",
       "4  (19.04, 22.64)              市單定加一大嘅唐州活發工程已經完成嘅嘞\n",
       "5  (22.64, 26.88)          而市建共亦都話嚟緊會引入共同租住單位嘅共居模式\n",
       "6     (0.0, 4.32)      希望底時呢度呢就可以變成一個充滿文化特色同埋活力嘅社區"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "df = pd.json_normalize(result, record_path =['chunks'])\n",
    "display(df)\n",
    "\n",
    "# show df in a tablular format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbf9d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
